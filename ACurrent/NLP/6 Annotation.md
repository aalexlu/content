*February 2, 2023*
previous: [[5 Text Classification 3 - Attention and transformers]]

---

## Transformers
Transforms map an input sequence of vectors to an output sequence of vectors of the same dimensionality
many attention blocks stacked

#### Self-attention
- transforms so you end up having a weighted average
- learning query, key, value
	- ex: comparing the query vector of 'The' with key vector of all other words
	- calculate weighted avg through value vectors
	- then run through softmax for 0-1

Residual layer adds the input of a layer to the output of a layer to preserve what went into it



---




next:
